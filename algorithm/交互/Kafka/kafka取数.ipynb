{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从kafka中不断取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "from confluent_kafka import Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mConsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A high-level Apache Kafka consumer\n",
       "\n",
       ".. py:function:: Consumer(config)\n",
       "\n",
       "Create a new Consumer instance using the provided configuration *dict* (including properties and callback functions). See :ref:`pythonclient_configuration` for more information.\n",
       "\n",
       ":param dict config: Configuration properties. At a minimum, ``group.id`` **must** be set and ``bootstrap.servers`` **should** be set.\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     DeserializingConsumer\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Consumer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "information = {}\n",
    "kafka = {}\n",
    "kafka['http'] = '192.168.50.110:19092'\n",
    "# kafka['topicCtrl'] = 'dc_abnormal_control'\n",
    "kafka['event_topic'] = 'dc_event'\n",
    "kafka['offset'] = 'earliest'\n",
    "kafka['groupid'] = 'AnomalyDetectTest01'\n",
    "information['kafka'] = kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kafka': {'http': '192.168.50.110:19092',\n",
       "  'topicData': 'dc_event',\n",
       "  'offset': 'earliest',\n",
       "  'groupid': 'AnomalyDetectTest01'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../infoamtion.json', 'w') as f:\n",
    "    json.dump(information, f, indent = 2, sort_keys = True, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer({\n",
    "    'bootstrap.servers':kafka['http'], # kafka所在ip地址\n",
    "    'group.id':time.time(),\n",
    "    'enable.auto.commit':True, # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "    'default.topic.config':{\n",
    "        'auto.offset.reset':kafka['offset']\n",
    "    }  \n",
    "})\n",
    "# c.subscribe([kafka['event_topic']]) # 为consumer分配分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.subscribe([kafka['event_topic']]) # 为consumer分配分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 没有break会不停的取数据\n",
    "while True:\n",
    "    msg = c.poll(100)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    else:\n",
    "        topicMsg = msg.topic()\n",
    "        message = json.loads(msg.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从kafka中取一段时间内的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError, TopicPartition, Message\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = {}\n",
    "kafka['http'] = '192.168.50.107:19092'\n",
    "kafka['topicCtrl'] = 'dc_abnormal_control'\n",
    "kafka['topicData'] = 'dc_abnormal_data'\n",
    "kafka['offset'] = 'earliest'\n",
    "kafka['groupid'] = 'AnomalyDetectTest01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer({\n",
    "    'bootstrap.servers':kafka['http'], # kafka所在ip地址\n",
    "    'group.id':time.time(),\n",
    "    'enable.auto.commit':True, # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "    'default.topic.config':{\n",
    "        'auto.offset.reset':kafka['offset']\n",
    "    }  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取昨天一天的数据，start_time、end_time这两个时间可以随便设置\n",
    "now = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "start_time = datetime.datetime.strptime(now.strftime('%Y-%m-%d 00:00:00'),'%Y-%m-%d %H:%M:%S')\n",
    "end_time = datetime.datetime.strptime(now.strftime('%Y-%m-%d 23:59:59'),'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前topic有多少个分区 \n",
    "cluster_data = c.list_topics(topic = kafka['topicData'])\n",
    "topic_data = cluster_data.topics[kafka['topicData']]\n",
    "available_partitions = topic_data.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tps = [TopicPartition(kafka['topicData'], index, int(start_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "start_offset = c.offsets_for_times(start_tps)\n",
    "end_tps = [TopicPartition(kafka['topicData'], index, int(end_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "end_offset = c.offsets_for_times(end_tps)\n",
    "c.assign(start_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopicPartition{topic=dc_abnormal_data,partition=0,offset=-1,error=None},\n",
       " TopicPartition{topic=dc_abnormal_data,partition=1,offset=38239,error=None},\n",
       " TopicPartition{topic=dc_abnormal_data,partition=2,offset=-1,error=None}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    if offset < end_offset[1].offset: # 有数据的在partition为1的topicpartition\n",
    "        kafka_timestamp = msg.timestamp()[1] # 搞数据入kafka的时间戳\n",
    "        kafka_value = json.loads(msg.value())\n",
    "        values.append(kafka_value)\n",
    "    else: \n",
    "        c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "        break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_offset[1].offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_json = {}\n",
    "values_to_json['values'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../values.json', 'w') as f:\n",
    "    json.dump(values_to_json, f, indent = 2, sort_keys = True, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从多个partition中取数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError, TopicPartition, Message\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = {}\n",
    "kafka['http'] = '192.168.50.107:19092'\n",
    "kafka['topicCtrl'] = 'dc_abnormal_control'\n",
    "kafka['topicData'] = 'dc_test_prophet'\n",
    "kafka['offset'] = 'earliest'\n",
    "kafka['groupid'] = 'AnomalyDetectTest01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer({\n",
    "    'bootstrap.servers':kafka['http'], # kafka所在ip地址\n",
    "    'group.id':time.time(),\n",
    "    'enable.auto.commit':True, # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "    'default.topic.config':{\n",
    "        'auto.offset.reset':kafka['offset']\n",
    "    }  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取昨天一天的数据，start_time、end_time这两个时间可以随便设置\n",
    "start = datetime.datetime.now() - datetime.timedelta(days=40)\n",
    "start_time = datetime.datetime.strptime(start.strftime('%Y-%m-%d 00:00:00'),'%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.now() - datetime.timedelta(days=40)\n",
    "end_time = datetime.datetime.strptime(end.strftime('%Y-%m-%d 23:59:59'),'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前topic有多少个分区 \n",
    "cluster_data = c.list_topics(topic = kafka['topicData'])\n",
    "topic_data = cluster_data.topics[kafka['topicData']]\n",
    "available_partitions = topic_data.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tps = [TopicPartition(kafka['topicData'], index, int(start_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "start_offset = c.offsets_for_times(start_tps)\n",
    "end_tps = [TopicPartition(kafka['topicData'], index, int(end_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "end_offset = c.offsets_for_times(end_tps)\n",
    "c.assign(start_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopicPartition{topic=dc_test_prophet,partition=0,offset=156,error=None},\n",
       " TopicPartition{topic=dc_test_prophet,partition=1,offset=130,error=None},\n",
       " TopicPartition{topic=dc_test_prophet,partition=2,offset=168,error=None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    if offset < end_offset[0].offset: \n",
    "        kafka_timestamp = msg.timestamp()[1]/1000 # 搞数据入kafka的时间戳\n",
    "        kafka_value = json.loads(msg.value())\n",
    "        values.append(kafka_value)\n",
    "    else: \n",
    "        c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "        break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    datapartitions = [i for i in end_offset if i.offset != -1]\n",
    "\n",
    "    for i in datapartitions:\n",
    "        if offset < i.offset: \n",
    "            kafka_timestamp = msg.timestamp()[1] # 搞数据入kafka的时间戳\n",
    "            kafka_value = json.loads(msg.value())\n",
    "            values.append(kafka_value)\n",
    "        else: \n",
    "            c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "            break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1649401570230)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = c.poll(100)\n",
    "msg.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n",
      "0.9999999999999982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weijunfei/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(normalize=True)\n",
    "\n",
    "# 创建数据集\n",
    "import numpy as np\n",
    "x = np.arange(10)    \n",
    "y = 2 * x + 1    \n",
    "X = x[:, np.newaxis]    \n",
    "model.fit( X, y )\n",
    "\n",
    "# 输出结果\n",
    "print( model.coef_ )    \n",
    "print( model.intercept_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHE = OneHotEncoder()    \n",
    "OHE.fit_transform( enc_DF ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
