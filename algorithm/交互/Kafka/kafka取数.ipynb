{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kafka 简单取数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "from confluent_kafka import Consumer, KafkaError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = {\n",
    "        \"http\": \"192.168.50.110:19092\",\n",
    "        \"offset\": \"earliest\",\n",
    "        \"groupid\": \"apm-rca3\",\n",
    "        \"topic.event\": \"dc_event\",\n",
    "        \"topic.rca\": \"dc_rca\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer(\n",
    "    {\n",
    "        \"bootstrap.servers\": kafka[\"http\"],  # kafka所在ip地址\n",
    "        \"group.id\": kafka[\"groupid\"],\n",
    "        \"enable.auto.commit\": True,          # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "        \"default.topic.config\": {\n",
    "            \"auto.offset.reset\": kafka[\"offset\"]\n",
    "        },\n",
    "    }\n",
    ")\n",
    "c.subscribe([kafka[\"topic.event\"]])           # 为consumer分配分区"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 没有break会不停的取数据\n",
    "while True:\n",
    "    msg = c.poll(100)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    else:\n",
    "        topicMsg = msg.topic()\n",
    "        message = json.loads(msg.value())\n",
    "        if (message['triggerTime'] >= 1687846200) and (message['triggerTime'] <= 1687846320) and (message['monitorId'] == 651):\n",
    "            print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从kafka中取一段时间内的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError, TopicPartition, Message\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = {}\n",
    "kafka['http'] = '192.168.50.107:19092'\n",
    "kafka['topicCtrl'] = 'dc_abnormal_control'\n",
    "kafka['topicData'] = 'dc_abnormal_data'\n",
    "kafka['offset'] = 'earliest'\n",
    "kafka['groupid'] = 'AnomalyDetectTest01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer({\n",
    "    'bootstrap.servers':kafka['http'], # kafka所在ip地址\n",
    "    'group.id':time.time(),\n",
    "    'enable.auto.commit':True, # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "    'default.topic.config':{\n",
    "        'auto.offset.reset':kafka['offset']\n",
    "    }  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取昨天一天的数据，start_time、end_time这两个时间可以随便设置\n",
    "now = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "start_time = datetime.datetime.strptime(now.strftime('%Y-%m-%d 00:00:00'),'%Y-%m-%d %H:%M:%S')\n",
    "end_time = datetime.datetime.strptime(now.strftime('%Y-%m-%d 23:59:59'),'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前topic有多少个分区 \n",
    "cluster_data = c.list_topics(topic = kafka['topicData'])\n",
    "topic_data = cluster_data.topics[kafka['topicData']]\n",
    "available_partitions = topic_data.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tps = [TopicPartition(kafka['topicData'], index, int(start_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "start_offset = c.offsets_for_times(start_tps)\n",
    "end_tps = [TopicPartition(kafka['topicData'], index, int(end_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "end_offset = c.offsets_for_times(end_tps)\n",
    "c.assign(start_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopicPartition{topic=dc_abnormal_data,partition=0,offset=-1,error=None},\n",
       " TopicPartition{topic=dc_abnormal_data,partition=1,offset=38239,error=None},\n",
       " TopicPartition{topic=dc_abnormal_data,partition=2,offset=-1,error=None}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    if offset < end_offset[1].offset: # 有数据的在partition为1的topicpartition\n",
    "        kafka_timestamp = msg.timestamp()[1] # 搞数据入kafka的时间戳\n",
    "        kafka_value = json.loads(msg.value())\n",
    "        values.append(kafka_value)\n",
    "    else: \n",
    "        c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "        break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38899"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_offset[1].offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_json = {}\n",
    "values_to_json['values'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../values.json', 'w') as f:\n",
    "    json.dump(values_to_json, f, indent = 2, sort_keys = True, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从多个partition中取数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError, TopicPartition, Message\n",
    "import datetime, time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = {}\n",
    "kafka['http'] = '192.168.50.107:19092'\n",
    "kafka['topicCtrl'] = 'dc_abnormal_control'\n",
    "kafka['topicData'] = 'dc_test_prophet'\n",
    "kafka['offset'] = 'earliest'\n",
    "kafka['groupid'] = 'AnomalyDetectTest01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Consumer({\n",
    "    'bootstrap.servers':kafka['http'], # kafka所在ip地址\n",
    "    'group.id':time.time(),\n",
    "    'enable.auto.commit':True, # 是否自动提交offset，设为True时，每隔一段时间就会提交一次offset\n",
    "    'default.topic.config':{\n",
    "        'auto.offset.reset':kafka['offset']\n",
    "    }  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉取昨天一天的数据，start_time、end_time这两个时间可以随便设置\n",
    "start = datetime.datetime.now() - datetime.timedelta(days=40)\n",
    "start_time = datetime.datetime.strptime(start.strftime('%Y-%m-%d 00:00:00'),'%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.now() - datetime.timedelta(days=40)\n",
    "end_time = datetime.datetime.strptime(end.strftime('%Y-%m-%d 23:59:59'),'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前topic有多少个分区 \n",
    "cluster_data = c.list_topics(topic = kafka['topicData'])\n",
    "topic_data = cluster_data.topics[kafka['topicData']]\n",
    "available_partitions = topic_data.partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tps = [TopicPartition(kafka['topicData'], index, int(start_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "start_offset = c.offsets_for_times(start_tps)\n",
    "end_tps = [TopicPartition(kafka['topicData'], index, int(end_time.timestamp() * 1000)) for index in range(len(available_partitions))]\n",
    "end_offset = c.offsets_for_times(end_tps)\n",
    "c.assign(start_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopicPartition{topic=dc_test_prophet,partition=0,offset=156,error=None},\n",
       " TopicPartition{topic=dc_test_prophet,partition=1,offset=130,error=None},\n",
       " TopicPartition{topic=dc_test_prophet,partition=2,offset=168,error=None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    if offset < end_offset[0].offset: \n",
    "        kafka_timestamp = msg.timestamp()[1]/1000 # 搞数据入kafka的时间戳\n",
    "        kafka_value = json.loads(msg.value())\n",
    "        values.append(kafka_value)\n",
    "    else: \n",
    "        c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "        break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "while True:\n",
    "    msg = c.poll(100) # poll：轮询一次过程中，在一定时间内broker可消费的数据，单位为ms，如这里的100,就是在0.1s内拉取数据返回到消费者端\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        prinit('Consumer error:{}'.format(msg.error()))\n",
    "        continue\n",
    "    offset = msg.offset()\n",
    "    datapartitions = [i for i in end_offset if i.offset != -1]\n",
    "\n",
    "    for i in datapartitions:\n",
    "        if offset < i.offset: \n",
    "            kafka_timestamp = msg.timestamp()[1] # 搞数据入kafka的时间戳\n",
    "            kafka_value = json.loads(msg.value())\n",
    "            values.append(kafka_value)\n",
    "        else: \n",
    "            c.unassign()  # 超过当前Partition的话，停止订阅\n",
    "            break\n",
    "c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1649401570230)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = c.poll(100)\n",
    "msg.timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
